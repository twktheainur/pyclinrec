<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>pyclinrec.recognizer.deep_recognizers API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pyclinrec.recognizer.deep_recognizers</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import re
from typing import Set, Tuple, List

import regex
import torch
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModel, AutoConfig

from pyclinrec.dictionary import DictionaryLoader
from pyclinrec.recognizer import ConceptRecognizer, Concept, Annotation
from pyclinrec.recognizer.intersection_recognizers import IntersectionConceptRecognizer
from transformers import BitsAndBytesConfig

from torch.utils.data import Dataset, DataLoader

class _LabelDataset(Dataset):
    def __init__(self, labels, concept_ids) -&gt; None:
        super().__init__()
        self.labels = labels
        self.concept_ids = concept_ids
        
    def __len__(self):
        return len(self.labels)
    
    def __getitem__(self, index):
        return self.labels[index], self.concept_ids[index]
    
    def get_concept_id(self, index):
        return self.concept_ids[index]
        

class IntersEmbeddingConceptRecognizer(IntersectionConceptRecognizer):

    def __init__(self, dictionary_loader: DictionaryLoader, stop_words_file: str, termination_terms_file: str, language: str,
                model_name_or_path: str, batch_size=32):
        super().__init__(dictionary_loader, stop_words_file, termination_terms_file, language)
        self.concept_token_vector_index = {}
        self.batch_size = batch_size
        
        nf4_config = BitsAndBytesConfig(
            load_in_4bit=False,
            bnb_4bit_quant_type=&#34;nf4&#34;,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_compute_dtype=torch.bfloat16
        )
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)
        self.model = AutoModel.from_pretrained(model_name_or_path, quantization_config=nf4_config)
        self.unk_token_id = self.tokenizer.unk_token_id
        self.cls_token_id = self.tokenizer.cls_token_id
        self.pad_token_id = self.tokenizer.pad_token_id
        self.eos_token_id = self.tokenizer.eos_token_id
        self.stop_words = self._piece_wise_tokenize_token_list(self.stop_words)
        self.termination_terms = self._piece_wise_tokenize_token_list(self.termination_terms)

    def initialize(self):
        print(&#34;Now loading the dictionary...&#34;)
        self.dictionary_loader.load()
        dictionary = self.dictionary_loader.dictionary  # type : List[DictionaryEntry]
        print(&#34;Now indexing the dictionary...&#34;)
        concept_labels = []
        concept_label_ids = []
        for entry in tqdm(list(dictionary), desc=&#34;Loading all labels&#34;):
            # we split concept ids from labels
            # fields = line.split(&#34;\t&#34;)
            label = entry.label
            concept_id = entry.id
            
            concept_labels.append(label)
            concept_label_ids.append(concept_id)

            if entry.synonyms:
                concept_labels.extend(entry.synonyms)
                concept_label_ids.append(concept_id)
            # self._load_concept_labels(concept_id, labels)
        
        dataset = _LabelDataset(concept_labels, concept_label_ids)
        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False, num_workers=8)
        for labels, concept_ids in tqdm(dataloader, desc=&#34;Embedding all labels&#34;):
            self._embed_batch_concept_labels(list(concept_ids), list(labels))

    def _piece_wise_tokenize_token_list(self, token_list):
        final_token_list = []

        for token in token_list:
            sub_tokens = self.tokenizer.tokenize(token)
            final_token_list.append(sub_tokens[-1])

        return self.tokenizer.convert_tokens_to_ids(final_token_list)

    def _embed_batch_concept_labels(self, concept_ids, labels):

        inputs = self.tokenizer(labels, max_length=512, padding=&#39;max_length&#39;, return_attention_mask=True, return_tensors=&#39;pt&#39;)
        tokens = inputs[&#39;input_ids&#39;]
        att_masks = inputs[&#39;attention_mask&#39;]
        last_tokens = [(att_mask == 0).nonzero()[0].item() for att_mask in att_masks]
        with torch.no_grad():
            model_output = self.model(**inputs)
            per_concept_label_indexes = {}
            for vector_index in range(model_output.last_hidden_state.shape[0]):
                    
                token_vectors = model_output[&#39;last_hidden_state&#39;][vector_index, 0:last_tokens[vector_index] - 1, :]
                concept_id = concept_ids[vector_index]
                if concept_id not in self.concept_index:
                    self.concept_index[concept_id] = Concept(concept_id)
                concept = self.concept_index[concept_id]
                concept.add_label(labels[vector_index], label_embedding=model_output[&#39;pooler_output&#39;])
                concept_token_count = 0
                if concept_id not in per_concept_label_indexes:
                    per_concept_label_indexes[concept_id] = 1
                else:
                    per_concept_label_indexes[concept_id] += 1
                key = f&#34;{str(concept_id)}:::{str(per_concept_label_indexes[concept_id])}&#34;
                last_token_index = last_tokens[vector_index] - 1
                if len(tokens.shape) &gt; 1:
                    tokens = tokens[vector_index,:last_token_index]
                else:
                    tokens = tokens[:last_token_index]
                for token in tokens:
                    token = token.item()
                    # We skip words that belong to the stop list and words that contain non alphanumerical characters
                    # we create the dictionary entry if it did not exist before
                    if token not in self.unigram_root_index:
                        self.unigram_root_index[token] = set()
                    # if it already existed we add the concept id to the corresponding set
                    self.unigram_root_index[token].add(key)

                    if token not in self.concept_token_vector_index:
                        self.concept_token_vector_index[token] = {}
                    self.concept_token_vector_index[token][concept_id] = token_vectors[concept_token_count, :]
                    concept_token_count += 1
                self.concept_length_index[key] = concept_token_count

    def _tokens_to_spans(self, tokens, text: str, initial_start_offset=0):
        spans = []  # type: List[Tuple[int, int, str]]
        start_offset = initial_start_offset
        end_offset = initial_start_offset

        for current_token_index in range(len(tokens)):
            raw_token = tokens[current_token_index]
            if &#34;#&#34; in raw_token:
                raw_token = raw_token.replace(&#34;#&#34;, &#34; &#34;).strip()
            end_offset += len(raw_token)
            span = (start_offset, end_offset, text[start_offset:end_offset])
            spans.append(span)
            if current_token_index &lt; len(tokens) - 1 and text[end_offset] == &#34; &#34;:
                offset = 0
                while end_offset + offset &lt; len(text) and text[end_offset + offset + 1] == &#34; &#34;:
                    offset += 1
                start_offset = end_offset + offset + 1
                end_offset += offset + 1
            else:
                start_offset = end_offset
        return spans

    def _concept_from_root(self, root) -&gt; Set[Concept]:
        if root not in self.unigram_root_index:
            return set()
        else:
            return self.unigram_root_index[root]
        
    def _root_function(self, token) -&gt; str:
        return token</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="pyclinrec.recognizer.deep_recognizers.IntersEmbeddingConceptRecognizer"><code class="flex name class">
<span>class <span class="ident">IntersEmbeddingConceptRecognizer</span></span>
<span>(</span><span>dictionary_loader: <a title="pyclinrec.dictionary.dictionary.DictionaryLoader" href="../dictionary/dictionary.html#pyclinrec.dictionary.dictionary.DictionaryLoader">DictionaryLoader</a>, stop_words_file: str, termination_terms_file: str, language: str, model_name_or_path: str, batch_size=32)</span>
</code></dt>
<dd>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p>
<h2 id="parameters">Parameters</h2>
<pre><code>dictionary_loader: DictionaryLoader
    The dictionary loader that will provide the dictionary contents
stop_words_file: str
    Path to a text file containing a list of stop words (one per line)
termination_terms_file: str
    Path to a text file containing a list of termination terms that stop the production of additional
    tokens in a matching mention.
language: str
    The language of the text that will processed (affects the choice of tokenizer and stemmer).
filters: List[AnnotationFilter]
    A list of filters to apply post recognition
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class IntersEmbeddingConceptRecognizer(IntersectionConceptRecognizer):

    def __init__(self, dictionary_loader: DictionaryLoader, stop_words_file: str, termination_terms_file: str, language: str,
                model_name_or_path: str, batch_size=32):
        super().__init__(dictionary_loader, stop_words_file, termination_terms_file, language)
        self.concept_token_vector_index = {}
        self.batch_size = batch_size
        
        nf4_config = BitsAndBytesConfig(
            load_in_4bit=False,
            bnb_4bit_quant_type=&#34;nf4&#34;,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_compute_dtype=torch.bfloat16
        )
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)
        self.model = AutoModel.from_pretrained(model_name_or_path, quantization_config=nf4_config)
        self.unk_token_id = self.tokenizer.unk_token_id
        self.cls_token_id = self.tokenizer.cls_token_id
        self.pad_token_id = self.tokenizer.pad_token_id
        self.eos_token_id = self.tokenizer.eos_token_id
        self.stop_words = self._piece_wise_tokenize_token_list(self.stop_words)
        self.termination_terms = self._piece_wise_tokenize_token_list(self.termination_terms)

    def initialize(self):
        print(&#34;Now loading the dictionary...&#34;)
        self.dictionary_loader.load()
        dictionary = self.dictionary_loader.dictionary  # type : List[DictionaryEntry]
        print(&#34;Now indexing the dictionary...&#34;)
        concept_labels = []
        concept_label_ids = []
        for entry in tqdm(list(dictionary), desc=&#34;Loading all labels&#34;):
            # we split concept ids from labels
            # fields = line.split(&#34;\t&#34;)
            label = entry.label
            concept_id = entry.id
            
            concept_labels.append(label)
            concept_label_ids.append(concept_id)

            if entry.synonyms:
                concept_labels.extend(entry.synonyms)
                concept_label_ids.append(concept_id)
            # self._load_concept_labels(concept_id, labels)
        
        dataset = _LabelDataset(concept_labels, concept_label_ids)
        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False, num_workers=8)
        for labels, concept_ids in tqdm(dataloader, desc=&#34;Embedding all labels&#34;):
            self._embed_batch_concept_labels(list(concept_ids), list(labels))

    def _piece_wise_tokenize_token_list(self, token_list):
        final_token_list = []

        for token in token_list:
            sub_tokens = self.tokenizer.tokenize(token)
            final_token_list.append(sub_tokens[-1])

        return self.tokenizer.convert_tokens_to_ids(final_token_list)

    def _embed_batch_concept_labels(self, concept_ids, labels):

        inputs = self.tokenizer(labels, max_length=512, padding=&#39;max_length&#39;, return_attention_mask=True, return_tensors=&#39;pt&#39;)
        tokens = inputs[&#39;input_ids&#39;]
        att_masks = inputs[&#39;attention_mask&#39;]
        last_tokens = [(att_mask == 0).nonzero()[0].item() for att_mask in att_masks]
        with torch.no_grad():
            model_output = self.model(**inputs)
            per_concept_label_indexes = {}
            for vector_index in range(model_output.last_hidden_state.shape[0]):
                    
                token_vectors = model_output[&#39;last_hidden_state&#39;][vector_index, 0:last_tokens[vector_index] - 1, :]
                concept_id = concept_ids[vector_index]
                if concept_id not in self.concept_index:
                    self.concept_index[concept_id] = Concept(concept_id)
                concept = self.concept_index[concept_id]
                concept.add_label(labels[vector_index], label_embedding=model_output[&#39;pooler_output&#39;])
                concept_token_count = 0
                if concept_id not in per_concept_label_indexes:
                    per_concept_label_indexes[concept_id] = 1
                else:
                    per_concept_label_indexes[concept_id] += 1
                key = f&#34;{str(concept_id)}:::{str(per_concept_label_indexes[concept_id])}&#34;
                last_token_index = last_tokens[vector_index] - 1
                if len(tokens.shape) &gt; 1:
                    tokens = tokens[vector_index,:last_token_index]
                else:
                    tokens = tokens[:last_token_index]
                for token in tokens:
                    token = token.item()
                    # We skip words that belong to the stop list and words that contain non alphanumerical characters
                    # we create the dictionary entry if it did not exist before
                    if token not in self.unigram_root_index:
                        self.unigram_root_index[token] = set()
                    # if it already existed we add the concept id to the corresponding set
                    self.unigram_root_index[token].add(key)

                    if token not in self.concept_token_vector_index:
                        self.concept_token_vector_index[token] = {}
                    self.concept_token_vector_index[token][concept_id] = token_vectors[concept_token_count, :]
                    concept_token_count += 1
                self.concept_length_index[key] = concept_token_count

    def _tokens_to_spans(self, tokens, text: str, initial_start_offset=0):
        spans = []  # type: List[Tuple[int, int, str]]
        start_offset = initial_start_offset
        end_offset = initial_start_offset

        for current_token_index in range(len(tokens)):
            raw_token = tokens[current_token_index]
            if &#34;#&#34; in raw_token:
                raw_token = raw_token.replace(&#34;#&#34;, &#34; &#34;).strip()
            end_offset += len(raw_token)
            span = (start_offset, end_offset, text[start_offset:end_offset])
            spans.append(span)
            if current_token_index &lt; len(tokens) - 1 and text[end_offset] == &#34; &#34;:
                offset = 0
                while end_offset + offset &lt; len(text) and text[end_offset + offset + 1] == &#34; &#34;:
                    offset += 1
                start_offset = end_offset + offset + 1
                end_offset += offset + 1
            else:
                start_offset = end_offset
        return spans

    def _concept_from_root(self, root) -&gt; Set[Concept]:
        if root not in self.unigram_root_index:
            return set()
        else:
            return self.unigram_root_index[root]
        
    def _root_function(self, token) -&gt; str:
        return token</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="pyclinrec.recognizer.intersection_recognizers.IntersectionConceptRecognizer" href="intersection_recognizers.html#pyclinrec.recognizer.intersection_recognizers.IntersectionConceptRecognizer">IntersectionConceptRecognizer</a></li>
<li><a title="pyclinrec.recognizer.recognizer.ConceptRecognizer" href="recognizer.html#pyclinrec.recognizer.recognizer.ConceptRecognizer">ConceptRecognizer</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="pyclinrec.recognizer.deep_recognizers.IntersEmbeddingConceptRecognizer.initialize"><code class="name flex">
<span>def <span class="ident">initialize</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize(self):
    print(&#34;Now loading the dictionary...&#34;)
    self.dictionary_loader.load()
    dictionary = self.dictionary_loader.dictionary  # type : List[DictionaryEntry]
    print(&#34;Now indexing the dictionary...&#34;)
    concept_labels = []
    concept_label_ids = []
    for entry in tqdm(list(dictionary), desc=&#34;Loading all labels&#34;):
        # we split concept ids from labels
        # fields = line.split(&#34;\t&#34;)
        label = entry.label
        concept_id = entry.id
        
        concept_labels.append(label)
        concept_label_ids.append(concept_id)

        if entry.synonyms:
            concept_labels.extend(entry.synonyms)
            concept_label_ids.append(concept_id)
        # self._load_concept_labels(concept_id, labels)
    
    dataset = _LabelDataset(concept_labels, concept_label_ids)
    dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False, num_workers=8)
    for labels, concept_ids in tqdm(dataloader, desc=&#34;Embedding all labels&#34;):
        self._embed_batch_concept_labels(list(concept_ids), list(labels))</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pyclinrec.recognizer" href="index.html">pyclinrec.recognizer</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="pyclinrec.recognizer.deep_recognizers.IntersEmbeddingConceptRecognizer" href="#pyclinrec.recognizer.deep_recognizers.IntersEmbeddingConceptRecognizer">IntersEmbeddingConceptRecognizer</a></code></h4>
<ul class="">
<li><code><a title="pyclinrec.recognizer.deep_recognizers.IntersEmbeddingConceptRecognizer.initialize" href="#pyclinrec.recognizer.deep_recognizers.IntersEmbeddingConceptRecognizer.initialize">initialize</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>